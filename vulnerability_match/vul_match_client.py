import json
import logging
import os.path

import pandas as pd
import requests

from tools.tool_global import read_data_file

def test_query_vul():
    product_list = [
        {"ip": "192.168.140.26", "brand": "hikvision", "product": "ds-7108ni-q1", "version": "4.30.300"},
        {"ip": "192.168.140.27", "brand": "hikvision", "product": "ds-2cd2121g1", "version": "v5.5.99 build200101"},
    ]
    url = "http://192.168.140.86:8000/vulmatch"
    vul_product_list = []
    r = requests.post(url, json=product_list, timeout=10)
    if r.status_code == 200:
        vul_product_list = r.json()
        for product in vul_product_list:
            print(product)
    elif r.status_code == 500:
        logging.error("server error!")
    else:
        logging.error(r.text)
    return vul_product_list


def query_vul(product_list, save_path):
    url = "http://ip:port/vulmatch"
    vul_product_list = []
    with open(save_path, "a+", encoding="utf-8") as f:
        r = requests.post(url, json=product_list, timeout=10)
        if r.status_code == 200:
            vul_product_list = r.json()
            for product in vul_product_list:
                f.write(json.dumps(product, ensure_ascii=False) + "\n")
                print(product)
        elif r.status_code == 500:
            logging.error("server error!")
        else:
            logging.error(r.text)
        # with open(save_path, "w", encoding="utf-8") as f:
        #     json.dump(vul_product_list, f, ensure_ascii=False, indent=4)
    # return vul_product_list


# def query_vul_by_batch(sample_path, batch_size):
#     query_list = []
def convert_data(data):
    result = []
    for item in data:
        label = item.get("new_label", item.get("label", []))
        if not label or label[2] in ["", 'nv', None, "null"]:
            continue

        result.append({"ip": item["ip"], "brand": label[0], "product": label[1], "version": label[2]})
    return result


def convert_data_to_excel(data, output_file="output.xlsx"):
    """
    Clean data and save as an Excel file (.xlsx)
    :param data: Original data list
    :param output_file: Output filename
    """
    result = []

    for item in data:
        # Prioritize getting 'new_label', otherwise get 'label'
        label = item.get("new_label", item.get("label", []))

        # Filtering logic:
        # 1. label is empty
        # 2. label length is less than 3 (prevent label[2] error)
        # 3. version number (index 2) is invalid
        if not label or len(label) < 3 or label[2] in ["", 'nv', None, "null"]:
            continue

        # Add each row as a dictionary to the result list
        # The dictionary Keys will automatically become Excel column headers
        result.append({
            "ip": item.get("ip"),
            "brand": label[0],
            "product": label[1],
            "version": label[2]
        })

    # Write to Excel using Pandas
    if result:
        df = pd.DataFrame(result)
        # index=False means do not save row indices (0, 1, 2...)
        df.to_excel(output_file, index=False)
        print(f"✅ Data successfully written to: {output_file} (Total {len(df)} records)")
    else:
        print("⚠️ No matching data found, file not generated.")

    return result


def convert_data_to_csv(data, output_file="device_info.csv"):
    """
    Clean data and save as a CSV file.
    Only fill 'IP Address', 'Brand', 'Model', 'Firmware Version', other predefined columns are automatically left empty.
    """

    # 1. Define the complete list of column names (please modify here to match all column names in your image)
    # Only columns matching keys in the 'row' dictionary below will be filled; others will be automatically empty.
    full_columns = [
        'IP Address',
        'Secondary Category',
        'Brand',
        'Model',
        'Order Number',
        'Firmware Version',
    ]

    processed_rows = []

    for item in data:
        # Get label data
        label = item.get("new_label", item.get("label", []))

        # Filter invalid data
        if not label or len(label) < 3 or label[2] in ["", 'nv', None, "null"]:
            continue

        # 2. Construct data rows, key names must exactly match those in full_columns
        row = {
            "IP Address": item.get("ip"),
            "Secondary Category": '',
            "Brand": label[0],
            "Model": label[1],
            "Order Number": '',
            "Firmware Version": label[2],
            # Other columns don't need to be written here; Pandas will automatically fill them with NaN (empty)
        }
        processed_rows.append(row)

    # 3. Generate CSV
    if processed_rows:
        df = pd.DataFrame(processed_rows)

        # Key step: Use reindex to enforce the complete column structure
        # This automatically creates columns like 'Port', 'Protocol', etc., and fills them with empty values (if added to full_columns)
        df = df.reindex(columns=full_columns)

        # Save file
        # encoding='utf-8-sig' solves the issue of garbled characters when opening CSV files containing non-ASCII characters in Excel
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"✅ Successfully exported CSV: {output_file} (Total {len(df)} records)")
    else:
        print("⚠️ No valid data to export")

    return processed_rows




if __name__ == "__main__":

    sample_path = r"F:\paper\Version_identification_using_LLMs\data_analysis_module\data\hasVersion\sample_restore\hasVersion_filtered_version_0509.json"
    data = read_data_file(sample_path)
    output_path = f"{os.path.basename(sample_path).split('.')[0]}_vul_new.jsonl"

    # convert_data_to_excel(data, output_path)
    # convert_data_to_csv(data, output_file=output_path)
    batch_size = 5000
    convert_data = convert_data(data)
    # for i in range(0, total_count, batch_size)
    total_count = len(convert_data)
    for i in range(0, total_count, batch_size):
        print(f"Processing batch {i} to {min(i + batch_size, total_count)} / {total_count}")
        batch_data = convert_data[i:i + batch_size]
        query_vul(batch_data, output_path)
    # query_vul(convert_data[5000:10000], output_path)
    print("done")
    # test_query_vul()
